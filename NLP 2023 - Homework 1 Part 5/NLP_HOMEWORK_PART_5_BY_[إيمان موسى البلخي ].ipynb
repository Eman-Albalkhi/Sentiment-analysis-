{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f519c8",
   "metadata": {
    "id": "46f519c8"
   },
   "source": [
    "#  إيمان موسى البلخي "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efe140",
   "metadata": {
    "id": "e7efe140"
   },
   "source": [
    "Prepare libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "af6058ed",
   "metadata": {
    "id": "af6058ed"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# here put every import you need e.g. import nltk\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pyarabic.arabrepr\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "import tashaphyne.arabic_const as arabconst\n",
    "import tashaphyne.arabic_const as arabconst\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import *\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import matplotlib.pyplot as plt           \n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import *\n",
    "import array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec48c7b",
   "metadata": {
    "id": "4ec48c7b"
   },
   "source": [
    "Prapere The Comparison Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0428b0f2",
   "metadata": {
    "id": "0428b0f2"
   },
   "outputs": [],
   "source": [
    "model_comparison_table = {}\n",
    "\n",
    "model_comparison_table['question_step_number'] = []\n",
    "model_comparison_table['model_name'] = []\n",
    "model_comparison_table['parameters'] = []\n",
    "model_comparison_table['preprocessing_methods'] = []\n",
    "model_comparison_table['accuracy'] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3400638b",
   "metadata": {
    "id": "3400638b"
   },
   "source": [
    "# Question [1]: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8E7xa9XLPW8t",
   "metadata": {
    "id": "8E7xa9XLPW8t"
   },
   "source": [
    "شرح ما يقوم به الكود"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f8de2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming uniformity\n",
    "def Naming_uniformity_label(lebel_tweet):\n",
    "    \n",
    "    lebel_tweet = lebel_tweet.lower()\n",
    "    lebel_tweet = re.sub(r'\\s', '', lebel_tweet)\n",
    "    lebel_tweet = re.sub(r'ir\\w+t', 'irrelevent', lebel_tweet)\n",
    "    lebel_tweet = re.sub(r'n\\w+l', 'neutral', lebel_tweet)\n",
    "    lebel_tweet = re.sub(r'n\\w+e', 'negative', lebel_tweet)\n",
    "    lebel_tweet = re.sub(r'p\\w+e', 'positive', lebel_tweet)\n",
    "    return lebel_tweet\n",
    "\n",
    "def Naming_uniformity_all_label(dataframe):\n",
    "    dataframe['label'] = dataframe['label'].apply(lambda x: \"\".join(preprocess_label(str(x))))\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d41a9612",
   "metadata": {
    "id": "d41a9612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_tweets_train:                                     33924\n",
      "\n",
      "Total_tweets after deleting lines containing NAN: 33633\n",
      "\n",
      "Total_tweets after deleting duplicate tweets:     26254\n",
      "************\n",
      "neutral       11559\n",
      "positive       5713\n",
      "negative       5472\n",
      "irrelevent     3510\n",
      "Name: label, dtype: int64\n",
      "************\n",
      "number tweets positive+negative:  (11185, 2)\n"
     ]
    }
   ],
   "source": [
    "# dataframe train\n",
    "dataframe_train = pd.read_csv('train.csv')\n",
    "print(\"Total_tweets_train:                                    \",dataframe_train.shape[0] )\n",
    "dataframe_train.dropna(inplace = True)\n",
    "print(\"\\nTotal_tweets after deleting lines containing NAN:\",dataframe_train.shape[0])\n",
    "dataframe=dataframe_train.drop_duplicates(subset=['text','label'],keep='first')\n",
    "dataframe_train=dataframe\n",
    "print(\"\\nTotal_tweets after deleting duplicate tweets:    \",dataframe_train.shape[0])\n",
    "\n",
    "\n",
    "dataframe_train=Naming_uniformity_all_label(dataframe_train)\n",
    "print('************')\n",
    "print(dataframe_train['label'].value_counts())\n",
    "print('************')\n",
    "\n",
    "\n",
    "dataframe_train_x=dataframe_train['text']\n",
    "dataframe_train_y=dataframe_train['label']\n",
    "\n",
    "# negative:0,positive:1,neutral:2,irrelevent:3\n",
    "for i in dataframe_train_y.index:\n",
    "    if dataframe_train_y[i]=='negative':\n",
    "         dataframe_train_y[i]=0\n",
    "    elif dataframe_train_y[i]=='positive':\n",
    "        dataframe_train_y[i]=1\n",
    "    elif dataframe_train_y[i]=='neutral':\n",
    "        dataframe_train_y[i]=2\n",
    "    elif dataframe_train_y[i]=='irrelevent':\n",
    "        dataframe_train_y[i]=3\n",
    "\n",
    "        \n",
    "# Only for positive and negative tweets\n",
    "dataframe_train_pn=dataframe_train[dataframe_train['label']<2]\n",
    "\n",
    "\n",
    "dataframe_train_pn_x=dataframe_train_pn['text']\n",
    "dataframe_train_pn_y=dataframe_train_pn['label']\n",
    "  \n",
    "print(\"number tweets positive+negative: \",dataframe_train_pn.shape)\n",
    "\n",
    "dataframe_train_x=dataframe_train['text']\n",
    "dataframe_train_y=dataframe_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "aNNrMjaaQanf",
   "metadata": {
    "id": "aNNrMjaaQanf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_tweets_vaild:                                     7269\n",
      "\n",
      "Total_tweets after deleting lines containing NAN: 7213\n",
      "\n",
      "Total_tweets after deleting duplicate tweets:     6815\n",
      "*********\n",
      "neutral       2980\n",
      "positive      1518\n",
      "negative      1403\n",
      "irrelevent     914\n",
      "Name: label, dtype: int64\n",
      "**********\n",
      "number tweets (positive+negative):  (2921, 2)\n"
     ]
    }
   ],
   "source": [
    "# dataframe vaild\n",
    "dataframe_vaild = pd.read_csv('valid.csv')\n",
    "print(\"Total_tweets_vaild:                                    \",dataframe_vaild.shape[0] )\n",
    "dataframe_vaild.dropna(inplace = True)\n",
    "print(\"\\nTotal_tweets after deleting lines containing NAN:\",dataframe_vaild.shape[0])\n",
    "dataframe=dataframe_vaild.drop_duplicates(subset=['text','label'],keep='first')\n",
    "dataframe_vaild=dataframe\n",
    "print(\"\\nTotal_tweets after deleting duplicate tweets:    \",dataframe_vaild.shape[0])\n",
    "\n",
    "# Naming uniformity\n",
    "dataframe_train=Naming_uniformity_all_label(dataframe_vaild)\n",
    "print(\"*********\")\n",
    "print(dataframe_vaild['label'].value_counts())\n",
    "print(\"**********\")\n",
    "\n",
    "dataframe_vaild_x=dataframe_vaild['text']\n",
    "dataframe_vaild_y=dataframe_vaild['label']\n",
    "\n",
    "# negative:0,positive:1,neutral:2,irrelevent:3\n",
    "for i in dataframe_vaild_y.index:\n",
    "    if dataframe_vaild_y[i]=='negative':\n",
    "         dataframe_vaild_y[i]=0\n",
    "    elif dataframe_vaild_y[i]=='positive':\n",
    "        dataframe_vaild_y[i]=1\n",
    "    elif dataframe_vaild_y[i]=='neutral':\n",
    "        dataframe_vaild_y[i]=2\n",
    "    elif dataframe_vaild_y[i]=='irrelevent':\n",
    "        dataframe_vaild_y[i]=3\n",
    "\n",
    "# Only for positive and negative tweets\n",
    "dataframe_vaild_pn=dataframe_vaild[dataframe_vaild['label']<2]\n",
    "\n",
    "dataframe_vaild_pn_x=dataframe_vaild_pn['text']\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn['label']\n",
    "\n",
    "print(\"number tweets (positive+negative): \",dataframe_vaild_pn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a19d9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_tweets_test:                                     7270\n",
      "\n",
      "Total_tweets after deleting lines containing NAN: 7204\n",
      "\n",
      "Total_tweets after deleting duplicate tweets:     6783\n",
      "*********\n",
      "neutral       2906\n",
      "positive      1510\n",
      "negative      1437\n",
      "irrelevent     930\n",
      "Name: label, dtype: int64\n",
      "**********\n",
      "number tweets positive+negative:  (2947, 2)\n"
     ]
    }
   ],
   "source": [
    "# datafram test\n",
    "dataframe_test = pd.read_csv('test.csv')\n",
    "print(\"Total_tweets_test:                                    \",dataframe_test.shape[0] )\n",
    "dataframe_test.dropna(inplace = True)\n",
    "print(\"\\nTotal_tweets after deleting lines containing NAN:\",dataframe_test.shape[0])\n",
    "dataframe=dataframe_test.drop_duplicates(subset=['text','label'],keep='first')\n",
    "dataframe_test=dataframe\n",
    "print(\"\\nTotal_tweets after deleting duplicate tweets:    \",dataframe_test.shape[0])\n",
    "\n",
    "# Naming uniformity\n",
    "dataframe_test=Naming_uniformity_all_label(dataframe_test)\n",
    "print(\"*********\")\n",
    "print(dataframe_test['label'].value_counts())\n",
    "print(\"**********\")\n",
    "\n",
    "dataframe_test_x=dataframe_test['text']\n",
    "dataframe_test_y=dataframe_test['label']\n",
    "\n",
    "# negative:0,positive:1,neutral:2,irrelevent:3\n",
    "for i in dataframe_test_y.index:\n",
    "    if dataframe_test_y[i]=='negative':\n",
    "         dataframe_test_y[i]=0\n",
    "    elif dataframe_test_y[i]=='positive':\n",
    "        dataframe_test_y[i]=1\n",
    "    elif dataframe_test_y[i]=='neutral':\n",
    "        dataframe_test_y[i]=2\n",
    "    elif dataframe_test_y[i]=='irrelevent':\n",
    "        dataframe_test_y[i]=3\n",
    "\n",
    "# Only for positive and negative tweets\n",
    "dataframe_test_pn=dataframe_test[dataframe_test['label']<2]\n",
    "\n",
    "dataframe_test_pn_x=dataframe_test_pn['text']\n",
    "dataframe_test_pn_y=dataframe_test_pn['label']\n",
    "\n",
    "print(\"number tweets positive+negative: \",dataframe_test_pn.shape)\n",
    "\n",
    "dataframe_test_pn_x=dataframe_test['text']\n",
    "dataframe_test_pn_y=dataframe_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c69c2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_tweets_train :  11185\n",
      "Total_tweets_vaild :  2921\n",
      "Total_tweets_test  :  2947\n"
     ]
    }
   ],
   "source": [
    "print(\"Total_tweets_train : \",dataframe_train_pn.shape[0])\n",
    "print(\"Total_tweets_vaild : \",dataframe_vaild_pn.shape[0])\n",
    "print(\"Total_tweets_test  : \",dataframe_test_pn.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22713d",
   "metadata": {
    "id": "2f22713d"
   },
   "source": [
    "# Question [2]: Baseline: Bag of Words with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef90d1",
   "metadata": {
    "id": "03ef90d1"
   },
   "source": [
    "شرح ما يقوم به الكود"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1cad9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train && vaild\n",
    "\n",
    "def BOW(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(dataframe_train_pn_x)\n",
    "X_vaild_counts = count_vectorizer.transform(dataframe_vaild_pn_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6cdc48f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "84e8da50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065730914070524"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "_iqs3mUtB8UV",
   "metadata": {
    "id": "_iqs3mUtB8UV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['2'],\n",
       " 'model_name': ['baseline logistic regression with bag of words'],\n",
       " 'parameters': ['default'],\n",
       " 'preprocessing_methods': ['none'],\n",
       " 'accuracy': [0.8065730914070524]}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['question_step_number'].append(\"2\")\n",
    "model_comparison_table['model_name'].append(\"baseline logistic regression with bag of words\")\n",
    "model_comparison_table['parameters'].append(\"default\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"none\")\n",
    "model_comparison_table['accuracy'].append(accuracy)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFmJ2HWqRNZa",
   "metadata": {
    "id": "oFmJ2HWqRNZa"
   },
   "source": [
    "# Question [3]: Text Cleaning and Normalization on Bag of Words with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyzKeB0QC8sY",
   "metadata": {
    "id": "lyzKeB0QC8sY"
   },
   "source": [
    "Prapere The Comparison Dictionary for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a87mEw9pC8sZ",
   "metadata": {
    "id": "a87mEw9pC8sZ"
   },
   "outputs": [],
   "source": [
    "bag_logistic_comparison_table = {}\n",
    "\n",
    "bag_logistic_comparison_table['question_step_number'] = []\n",
    "bag_logistic_comparison_table['preprocessing_methods'] = []\n",
    "bag_logistic_comparison_table['accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwaCNfS9RNZc",
   "metadata": {
    "id": "cwaCNfS9RNZc"
   },
   "source": [
    "## [3.1] \n",
    "شرح ما يقوم به الكود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "79317f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_links(text ): \n",
    "    text = re.sub(r'https?\\/\\/S+', '',  text) # remove the hyperlink\n",
    "    text = re.sub(r'http\\S+', '',  text) # remove the hyperlink\n",
    "    text = re.sub(r'www\\S+', '',  text) # remove the www\n",
    "    return text\n",
    "\n",
    "def delete_mentions(text): \n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "\n",
    "def delete_hashtags1(text):\n",
    "    return re.sub(r\"#(\\S+)\",'', text)\n",
    "\n",
    "def delete_hashtags2(text):\n",
    "    return re.sub(r\"#\",'', text)\n",
    "\n",
    "def Delete_duplicate_characters(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def replace_Hindi_numbers_to_Arabic_number(text):\n",
    "    text=re.sub(r'٠', r'0', text)\n",
    "    text=re.sub(r'١', r'1', text)\n",
    "    text=re.sub(r'٢', r'2', text)\n",
    "    text=re.sub(r'٣', r'3', text)\n",
    "    text=re.sub(r'٤', r'4', text)\n",
    "    text=re.sub(r'٥', r'5', text)\n",
    "    text=re.sub(r'٦', r'6', text)\n",
    "    text=re.sub(r'٧', r'7', text)\n",
    "    text=re.sub(r'٨', r'8', text)\n",
    "    text=re.sub(r'٩', r'9', text)\n",
    "    return text \n",
    "\n",
    "def standardize_number(text):\n",
    "    return re.sub(r'\\d+','*',text)\n",
    "\n",
    "def delete_all_number(text):\n",
    "    return re.sub(r'\\d+','',text)\n",
    "def delete_punctuation_marks(text): \n",
    "    text = re.sub(r'\\n|[!\\\"\\＄%&\\'\\(\\)\\*\\+,-\\./:;<=>\\?\\[\\\\\\]\\^`{\\|}~،“؟”]^#(\\S+)', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def delete_imojis(text): \n",
    "    imoji_regex=re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = re.sub(imoji_regex ,' ', text);\n",
    "    return text\n",
    "\n",
    "def delete_NON_Arabics(text): \n",
    "    Regex_non_arabic=r'\\d+|[\\u0660-\\u0669]|[\\!\\\"\\＄%&\\'\\(\\)\\*\\+,-\\./:;<=>\\?\\[\\\\\\]\\^`{\\|}~،“؟”]|#|_|[\\u0600-\\u06FF]|\\s|[u\"\\U0001F600-\\U0001F64F\"\"\\U0001F300-\\U0001F5FF\"u\"\\U0001F680-\\U0001F6FF\"u\"\\U0001F1E0-\\U0001F1FF\"u\"\\U00002702-\\U000027B0\"u\"\\U000024C2-\\U0001F251\"]+'\n",
    "    arabic_words = \"\".join(re.findall(Regex_non_arabic, text))\n",
    "    return arabic_words\n",
    "\n",
    "def stop_word(text):\n",
    "    text = [w for w in text if w not in stopwords.words(\"arabic\")]\n",
    "\n",
    "\n",
    "def Stemming_English(text_token):\n",
    "    lancaster = nltk.LancasterStemmer() \n",
    "    return [(t,lancaster.stem(t)) for t in text_token]\n",
    "\n",
    "def Stemming_Arabic (text):\n",
    "    text=nltk.word_tokenize(text)\n",
    "    ArListem = ArabicLightStemmer()    \n",
    "    return [(t,ArListem.light_stem(t)) for t in text]\n",
    "\n",
    "def normalize_hamza(text):\n",
    "    arabconst.ALEFAT_PAT.sub(arabconst.ALEF, text)\n",
    "    return arabconst.HAMZAT_PAT.sub(arabconst.HAMZA, text)\n",
    "\n",
    "def normalize_ALEFAT(text):\n",
    "    return arabconst.ALEFAT_PAT.sub(arabconst.ALEF, text)\n",
    "def strip_tatweel(text):\n",
    "    return re.sub(u'[%s]' % arabconst.TATWEEL, '', text)\n",
    "def strip_tashkeel(text):\n",
    "    return arabconst.HARAKAT_PAT.sub('', text)\n",
    "def replace_multiple_spaces_with_a_single_space(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "def delete_Frequency__tweet(list_tweet):\n",
    "    return set(list_tweet)\n",
    "\n",
    "def preprocess(text,\n",
    "               delete_link ,\n",
    "               delete_mention , \n",
    "               delete_hashtag1 ,\n",
    "               delete_hashtag2,\n",
    "               Delete_duplicate_character,\n",
    "               standardize_num,\n",
    "               delete_all_num,\n",
    "               delete_punctuation_mark,\n",
    "               delete_imoji,\n",
    "               delete_nonArabic,\n",
    "               stop__word,\n",
    "               Stemming_Englis,\n",
    "               Stemming_Arabi,\n",
    "               normalize_hamz,\n",
    "               normalize_ALEFA,\n",
    "               strip_tatwee,\n",
    "               strip_tashkee,\n",
    "               replace_multiple_spaces_with_a_single_spac,\n",
    "               delete_Frequency__twee ):\n",
    "    \n",
    "    if delete_link==1:\n",
    "        text=delete_links(text)\n",
    "        \n",
    "    if delete_mention==1:\n",
    "        text=delete_mentions(text)\n",
    "        \n",
    "    if delete_hashtag1==1:\n",
    "        text=delete_hashtags1(text)\n",
    "        \n",
    "    if delete_hashtag2==1:\n",
    "        text=delete_hashtags2(text)\n",
    "        \n",
    "    if Delete_duplicate_character==1:\n",
    "        text=Delete_duplicate_characters(text)\n",
    "        \n",
    "    if standardize_num==1:\n",
    "        text=standardize_number(text)\n",
    "        \n",
    "    if delete_all_num==1:\n",
    "        text=delete_all_number(text)\n",
    "        \n",
    "    if delete_punctuation_mark==1:\n",
    "        text=delete_punctuation_marks(text)\n",
    "        \n",
    "    if delete_imoji==1:\n",
    "        text=delete_imojis(text)\n",
    "        \n",
    "    if delete_nonArabic==1:\n",
    "        text=delete_NON_Arabics(text)\n",
    "        \n",
    "    if stop__word==1:\n",
    "        text=stop_word(text)\n",
    "        \n",
    "    if Stemming_Englis==1:\n",
    "        text=Stemming_English(text)\n",
    "        \n",
    "    if Stemming_Arabi==1:\n",
    "        text=Stemming_Arabic(text)\n",
    "        \n",
    "    if normalize_hamz==1:\n",
    "        text=normalize_hamza(text)\n",
    "        \n",
    "    if normalize_ALEFA==1:\n",
    "        text=normalize_ALEFAT(text)\n",
    "        \n",
    "    if strip_tatwee==1:\n",
    "        text=strip_tatweel(text)\n",
    "        \n",
    "    if strip_tashkee==1:\n",
    "        text=strip_tashkeel(text)\n",
    "        \n",
    "    if replace_multiple_spaces_with_a_single_spac==1:\n",
    "        text=replace_multiple_spaces_with_a_single_space(text)\n",
    "        \n",
    "    if delete_Frequency__twee==1:\n",
    "        text=delete_Frequency__tweet(text)   \n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "TCMARMvlCs9l",
   "metadata": {
    "id": "TCMARMvlCs9l"
   },
   "outputs": [],
   "source": [
    "trainX_delete_link_delete_mention  = [preprocess(t ,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_link_delete_mention  = [preprocess(t ,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "60144571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train_counts, count_vectorizer = BOW(trainX_delete_link_delete_mention)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_link_delete_mention)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "Fe1FkdufCs9p",
   "metadata": {
    "id": "Fe1FkdufCs9p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052036973639165"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your model\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57b3ef",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "إزالة الروابط والمنشن ادى الى خفض الدقة  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "YLTsifTUCs9q",
   "metadata": {
    "id": "YLTsifTUCs9q"
   },
   "outputs": [],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.1\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"delete link +delete_mention\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MbJeF_CxRNZd",
   "metadata": {
    "id": "MbJeF_CxRNZd"
   },
   "source": [
    "## [3.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "X6yszO6ODdQ8",
   "metadata": {
    "id": "X6yszO6ODdQ8"
   },
   "outputs": [],
   "source": [
    "trainX_delete_hashtags1 = [preprocess(t ,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_hashtags1 = [preprocess(t ,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "900313ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train_counts, count_vectorizer = BOW(trainX_delete_hashtags1)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_hashtags1)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "19LpA-DtDdQ9",
   "metadata": {
    "id": "19LpA-DtDdQ9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000684697021568"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your model\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782edf2",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "هنالك تابعين لازالة الهاشتاك  \n",
    "الاول يحذف الكلمات المرفقة مع الهاشتاك \n",
    "وعند تطبيق هذا التابع ادى الى خفض الدقة "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "qx7jADUHDdQ9",
   "metadata": {
    "id": "qx7jADUHDdQ9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1', '3.2.1'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention', 'remove hashtags1'],\n",
       " 'accuracy': [0.8052036973639165, 0.8000684697021568]}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.2.1\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"remove hashtags1\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8d6fb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_delete_hashtags2 = [preprocess(t ,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_hashtags2 = [preprocess(t ,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "38088cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train_counts, count_vectorizer = BOW(trainX_delete_hashtags2)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_hashtags2)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "009da29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8069154399178363"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2120503",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "ازالة الهاشتاغ عن طريق التابع الثاني الذي يحذف فقظ اشارة #\n",
    "\n",
    "ادى هذا التابع الى رفع الدقة "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4e24bff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1', '3.2.1', '3.2.2'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2'],\n",
       " 'accuracy': [0.8052036973639165, 0.8000684697021568, 0.8069154399178363]}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.2.2\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"remove hashtags2\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GY-HhD2YRNZe",
   "metadata": {
    "id": "GY-HhD2YRNZe"
   },
   "source": [
    "## [3.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "l-5TSuHWRNZe",
   "metadata": {
    "id": "l-5TSuHWRNZe"
   },
   "outputs": [],
   "source": [
    "trainX_delete_Delete_duplicate_characters= [preprocess(t ,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_Delete_duplicate_characters = [preprocess(t ,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5d4666d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train_counts, count_vectorizer = BOW(trainX_delete_Delete_duplicate_characters)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_Delete_duplicate_characters)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b9067a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017802122560767"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ddd974f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1', '3.2.1', '3.2.2', '3.3'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767]}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.3\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"Delete duplicate characters\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cddf849",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "ادى حذف المحارف المكررة من النص الى خفض الدقة  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KV9D86kbRNZe",
   "metadata": {
    "id": "KV9D86kbRNZe"
   },
   "source": [
    "## [3.4] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fM_FPldSCxp",
   "metadata": {
    "id": "3fM_FPldSCxp"
   },
   "source": [
    "### [3.4.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ohGu5R14RNZe",
   "metadata": {
    "id": "ohGu5R14RNZe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8113659705580281"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_replace_Hindi_numbers_to_Arabic_number = [preprocess(t ,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_replace_Hindi_numbers_to_Arabic_number = [preprocess(t ,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_replace_Hindi_numbers_to_Arabic_number)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_replace_Hindi_numbers_to_Arabic_number)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3dabba",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "تحويل الارقام الهندية لارقام عربية ادى الى رفع الدقة "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cf1f2f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1', '3.2.1', '3.2.2', '3.3', '3.4.1'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281]}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.4.1\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"replace Hindi numbers to Arabic number\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N2z1KmQXSAFs",
   "metadata": {
    "id": "N2z1KmQXSAFs"
   },
   "source": [
    "### [3.4.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fmOXIMWoSAFt",
   "metadata": {
    "id": "fmOXIMWoSAFt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8096542280041081"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_standardize_number = [preprocess(t ,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_standardize_number = [preprocess(t ,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_standardize_number)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_standardize_number)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "14356d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1', '3.2.1', '3.2.2', '3.3', '3.4.1', '3.4.2'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081]}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.4.2\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"standardize number\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd71a0",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "توحيد كتابة الارقام واستبدالها بنجمة ادى الى رفع الدقة ولكن تحويل الارقام الهندية الى ارقام عربية رفع الدقة اكثر\n",
    "\n",
    "مما يوضح أهمية الارقام في التغريديات "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fCBHc2y6SAL5",
   "metadata": {
    "id": "fCBHc2y6SAL5"
   },
   "source": [
    "### [3.4.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "QJX_PtSzSAL6",
   "metadata": {
    "id": "QJX_PtSzSAL6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8065730914070524"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_delete_all_number = [preprocess(t ,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_all_number = [preprocess(t ,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_delete_all_number)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_all_number)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f5fd7437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524]}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.4.3\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"delete all number\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de99b0",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف الارقام كلياا من التغريدات كان اسوء حالة \n",
    "\n",
    "من بين الحالات الثلاثة السابقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P1aLjPbzRNZe",
   "metadata": {
    "id": "P1aLjPbzRNZe"
   },
   "source": [
    "## [3.5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbqN7BceSRhS",
   "metadata": {
    "id": "dbqN7BceSRhS"
   },
   "source": [
    "### [3.5.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "caLTQ8EtSRhT",
   "metadata": {
    "id": "caLTQ8EtSRhT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8079424854501883"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_delete_NON_Arabics = [preprocess(t ,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_NON_Arabics = [preprocess(t ,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_delete_NON_Arabics)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_NON_Arabics)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2b16c557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883]}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.5.1\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"delete NON Arabics\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7299aa2",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف المحارف غير العربية ادى الى رفع الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rad8wPeSSRhU",
   "metadata": {
    "id": "rad8wPeSSRhU"
   },
   "source": [
    "### [3.5.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "UZuYtSHaSRhU",
   "metadata": {
    "id": "UZuYtSHaSRhU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8065730914070524"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainX_delete_punctuation_marks = [preprocess(t ,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_punctuation_marks = [preprocess(t ,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_delete_punctuation_marks)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_punctuation_marks)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d458740c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks '],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524]}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.5.2\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"delete punctuation marks \")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89029041",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف علامات الترقيم جميعها لم يؤثر على الدقة  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FbIUIkezSRhU",
   "metadata": {
    "id": "FbIUIkezSRhU"
   },
   "source": [
    "### [3.5.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "otquvQjuSRhV",
   "metadata": {
    "id": "otquvQjuSRhV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8069154399178363"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainX_delete_imojis = [preprocess(t ,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_delete_imojis = [preprocess(t ,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_delete_imojis)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_delete_imojis)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f39ca3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis '],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363]}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.5.3\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"delete imojis \")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a32528",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف الوجوه التعبيرية ادى لرفع الدقة  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N4WDktTgRNZf",
   "metadata": {
    "id": "N4WDktTgRNZf"
   },
   "source": [
    "## [3.6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "B6icsYsFRNZf",
   "metadata": {
    "id": "B6icsYsFRNZf"
   },
   "outputs": [],
   "source": [
    "# trainX_stop__word = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "# vaildX_stop__word = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "# X_train_counts, count_vectorizer = BOW(trainX_stop__word)\n",
    "# X_vaild_counts = count_vectorizer.transform(vaildX_stop__word)\n",
    "\n",
    "# clf = LogisticRegression(random_state=40)\n",
    "# dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "# clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "# y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "# dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "# accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "18467c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0.8060230914070721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bc9980cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word '],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721]}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.6\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"stop word \")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a102d",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف كلمات التوقف ادى لخفض الدقة\n",
    "\n",
    "تم تخزين الدقة في المتحول السابق وتعليق الكود لان تنفيذ هالتابع كتير بطيئ وعم ياخذ وقت كتير بسب انو هو عم بشيك على كل تغريدة على حدى وبقارنها مع كل كلمات التوقف "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wbGPWNtoR4p-",
   "metadata": {
    "id": "wbGPWNtoR4p-"
   },
   "source": [
    "## [3.7] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fAiyofe3R4qA",
   "metadata": {
    "id": "fAiyofe3R4qA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# trainX_Stemming_Arabic = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "# vaildX_Stemming_Arabic = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "# X_train_counts, count_vectorizer = BOW(trainX_Stemming_Arabic)\n",
    "# X_vaild_counts = count_vectorizer.transform(vaildX_Stemming_Arabic)\n",
    "\n",
    "# clf = LogisticRegression(random_state=40)\n",
    "# dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "# clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "# y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "# dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "# accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "73d98b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5dc2a1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6',\n",
       "  '3.7'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word ',\n",
       "  'Stemming Arabic'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721,\n",
       "  0]}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.7\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"Stemming Arabic\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V-rCzzoMR41T",
   "metadata": {
    "id": "V-rCzzoMR41T"
   },
   "source": [
    "## [3.8] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iXF2L4Z-Sd3N",
   "metadata": {
    "id": "iXF2L4Z-Sd3N"
   },
   "source": [
    "### [3.8.1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "wk-Z3Gw4Sd3O",
   "metadata": {
    "id": "wk-Z3Gw4Sd3O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8058883943854844"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainX_normalize_hamz = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_normalize_hamz = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_normalize_hamz)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_normalize_hamz)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b75d9216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6',\n",
       "  '3.7',\n",
       "  '3.8.1'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word ',\n",
       "  'Stemming Arabic',\n",
       "  'normalize hamz'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721,\n",
       "  0,\n",
       "  0.8058883943854844]}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.8.1\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"normalize hamz\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7faa1",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "توحيد كتابة الهمزات ادى لخفض الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ij5gdJA4Sd3O",
   "metadata": {
    "id": "ij5gdJA4Sd3O"
   },
   "source": [
    "### [3.8.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bDLOsd4tSd3O",
   "metadata": {
    "id": "bDLOsd4tSd3O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8034919548099966"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainX_normalize_ALEFA = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_normalize_ALEFA = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_normalize_ALEFA)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_normalize_ALEFA)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ca3f172e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6',\n",
       "  '3.7',\n",
       "  '3.8.1',\n",
       "  '3.8.2'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word ',\n",
       "  'Stemming Arabic',\n",
       "  'normalize hamz',\n",
       "  'normalize ALEFA'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721,\n",
       "  0,\n",
       "  0.8058883943854844,\n",
       "  0.8034919548099966]}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.8.2\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"normalize ALEFA\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c7084",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "توحيد الألفات ادى لخفض الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H6k8JnJDSd3P",
   "metadata": {
    "id": "H6k8JnJDSd3P"
   },
   "source": [
    "### [3.8.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "JhSOCYLmSd3P",
   "metadata": {
    "id": "JhSOCYLmSd3P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8052036973639165"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainX_strip_tatwee = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_strip_tatwee = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_strip_tatwee)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_strip_tatwee)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2d22ab89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6',\n",
       "  '3.7',\n",
       "  '3.8.1',\n",
       "  '3.8.2',\n",
       "  '3.8.3'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word ',\n",
       "  'Stemming Arabic',\n",
       "  'normalize hamz',\n",
       "  'normalize ALEFA',\n",
       "  'strip tatwee'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721,\n",
       "  0,\n",
       "  0.8058883943854844,\n",
       "  0.8034919548099966,\n",
       "  0.8052036973639165]}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.8.3\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"strip tatwee\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c3ccc",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف التطويل ادى لخفض الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1TmDb343Sls9",
   "metadata": {
    "id": "1TmDb343Sls9"
   },
   "source": [
    "### [3.8.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "wWJ-Z11_Sls-",
   "metadata": {
    "id": "wWJ-Z11_Sls-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8052036973639165"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_strip_tashkee = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_strip_tashkee = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_strip_tatwee)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_strip_tatwee)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a139f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6',\n",
       "  '3.7',\n",
       "  '3.8.1',\n",
       "  '3.8.2',\n",
       "  '3.8.3',\n",
       "  '3.8.4'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word ',\n",
       "  'Stemming Arabic',\n",
       "  'normalize hamz',\n",
       "  'normalize ALEFA',\n",
       "  'strip tatwee',\n",
       "  'strip tashkee'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721,\n",
       "  0,\n",
       "  0.8058883943854844,\n",
       "  0.8034919548099966,\n",
       "  0.8052036973639165,\n",
       "  0.8052036973639165]}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.8.4\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"strip tashkee\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db477d",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "حذف علامات التشكيل ادى لخفض الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0V5LD9EWVgY7",
   "metadata": {
    "id": "0V5LD9EWVgY7"
   },
   "source": [
    "## [3.9] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "AkVzG3AWVgZA",
   "metadata": {
    "id": "AkVzG3AWVgZA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8065730914070524"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_replace_multiple_spaces_with_a_single_spac = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_replace_multiple_spaces_with_a_single_spac = [preprocess(t ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0) for t in dataframe_vaild_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_replace_multiple_spaces_with_a_single_spac)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_replace_multiple_spaces_with_a_single_spac)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "94b8f7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['3.1',\n",
       "  '3.2.1',\n",
       "  '3.2.2',\n",
       "  '3.3',\n",
       "  '3.4.1',\n",
       "  '3.4.2',\n",
       "  '3.4.3',\n",
       "  '3.5.1',\n",
       "  '3.5.2',\n",
       "  '3.5.3',\n",
       "  '3.6',\n",
       "  '3.7',\n",
       "  '3.8.1',\n",
       "  '3.8.2',\n",
       "  '3.8.3',\n",
       "  '3.8.4',\n",
       "  '3.9'],\n",
       " 'preprocessing_methods': ['delete link +delete_mention',\n",
       "  'remove hashtags1',\n",
       "  'remove hashtags2',\n",
       "  'Delete duplicate characters',\n",
       "  'replace Hindi numbers to Arabic number',\n",
       "  'standardize number',\n",
       "  'delete all number',\n",
       "  'delete NON Arabics',\n",
       "  'delete punctuation marks ',\n",
       "  'delete imojis ',\n",
       "  'stop word ',\n",
       "  'Stemming Arabic',\n",
       "  'normalize hamz',\n",
       "  'normalize ALEFA',\n",
       "  'strip tatwee',\n",
       "  'strip tashkee',\n",
       "  'replace multiple spaces with a single spac'],\n",
       " 'accuracy': [0.8052036973639165,\n",
       "  0.8000684697021568,\n",
       "  0.8069154399178363,\n",
       "  0.8017802122560767,\n",
       "  0.8113659705580281,\n",
       "  0.8096542280041081,\n",
       "  0.8065730914070524,\n",
       "  0.8079424854501883,\n",
       "  0.8065730914070524,\n",
       "  0.8069154399178363,\n",
       "  0.8060230914070721,\n",
       "  0,\n",
       "  0.8058883943854844,\n",
       "  0.8034919548099966,\n",
       "  0.8052036973639165,\n",
       "  0.8052036973639165,\n",
       "  0.8065730914070524]}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_logistic_comparison_table['question_step_number'].append(\"3.9\")\n",
    "bag_logistic_comparison_table['preprocessing_methods'].append(\"replace multiple spaces with a single spac\")\n",
    "bag_logistic_comparison_table['accuracy'].append(accuracy)\n",
    "bag_logistic_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfcce1",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    " حذف الفراغات المتكررة واستبدالها بفراغ واحد لم يؤثر على الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vc96CuQStFA9",
   "metadata": {
    "id": "vc96CuQStFA9"
   },
   "source": [
    "## [3.10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fba8bf",
   "metadata": {
    "direction": "rtl",
    "id": "pCJ7itd3tJjw"
   },
   "source": [
    "التوابع الاضافية كانت تابع الهاشتغ الذي يقوم بحذف فقط اشارة الهاشتغ وتم تجريبة في الطلب الثاني \n",
    "\n",
    "وادى لرفع الدقة "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lSaKeo_BEuWK",
   "metadata": {
    "id": "lSaKeo_BEuWK"
   },
   "source": [
    "## Print Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "A_3K2Wz9EuWK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "A_3K2Wz9EuWK",
    "outputId": "f382a09e-4322-4923-e7a0-93964a9507b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_step_number</th>\n",
       "      <th>preprocessing_methods</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.1</td>\n",
       "      <td>delete link +delete_mention</td>\n",
       "      <td>0.805204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.2.1</td>\n",
       "      <td>remove hashtags1</td>\n",
       "      <td>0.800068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2.2</td>\n",
       "      <td>remove hashtags2</td>\n",
       "      <td>0.806915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.3</td>\n",
       "      <td>Delete duplicate characters</td>\n",
       "      <td>0.801780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.4.1</td>\n",
       "      <td>replace Hindi numbers to Arabic number</td>\n",
       "      <td>0.811366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.4.2</td>\n",
       "      <td>standardize number</td>\n",
       "      <td>0.809654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.4.3</td>\n",
       "      <td>delete all number</td>\n",
       "      <td>0.806573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.5.1</td>\n",
       "      <td>delete NON Arabics</td>\n",
       "      <td>0.807942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.5.2</td>\n",
       "      <td>delete punctuation marks</td>\n",
       "      <td>0.806573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.5.3</td>\n",
       "      <td>delete imojis</td>\n",
       "      <td>0.806915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.6</td>\n",
       "      <td>stop word</td>\n",
       "      <td>0.806023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.7</td>\n",
       "      <td>Stemming Arabic</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.8.1</td>\n",
       "      <td>normalize hamz</td>\n",
       "      <td>0.805888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.8.2</td>\n",
       "      <td>normalize ALEFA</td>\n",
       "      <td>0.803492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.8.3</td>\n",
       "      <td>strip tatwee</td>\n",
       "      <td>0.805204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.8.4</td>\n",
       "      <td>strip tashkee</td>\n",
       "      <td>0.805204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.9</td>\n",
       "      <td>replace multiple spaces with a single spac</td>\n",
       "      <td>0.806573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_step_number                       preprocessing_methods  accuracy\n",
       "0                   3.1                 delete link +delete_mention  0.805204\n",
       "1                 3.2.1                            remove hashtags1  0.800068\n",
       "2                 3.2.2                            remove hashtags2  0.806915\n",
       "3                   3.3                 Delete duplicate characters  0.801780\n",
       "4                 3.4.1      replace Hindi numbers to Arabic number  0.811366\n",
       "5                 3.4.2                          standardize number  0.809654\n",
       "6                 3.4.3                           delete all number  0.806573\n",
       "7                 3.5.1                          delete NON Arabics  0.807942\n",
       "8                 3.5.2                   delete punctuation marks   0.806573\n",
       "9                 3.5.3                              delete imojis   0.806915\n",
       "10                  3.6                                  stop word   0.806023\n",
       "11                  3.7                             Stemming Arabic  0.000000\n",
       "12                3.8.1                              normalize hamz  0.805888\n",
       "13                3.8.2                             normalize ALEFA  0.803492\n",
       "14                3.8.3                                strip tatwee  0.805204\n",
       "15                3.8.4                               strip tashkee  0.805204\n",
       "16                  3.9  replace multiple spaces with a single spac  0.806573"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(bag_logistic_comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5egzCqh_Dvrf",
   "metadata": {
    "direction": "rtl",
    "id": "5egzCqh_Dvrf"
   },
   "source": [
    "## Your best model\n",
    "ما هي العمليات الأفضل؟ اكتبها هنا\n",
    "\n",
    "ازالة الهاشتغ (حذف فقط الاشارة)\n",
    "\n",
    "تحويل الارقام الهندية للارقام العربية وتبين ان هنالك اهمية لوجود الارقام في التغريدات\n",
    "\n",
    "حذف المحارف غير العربية\n",
    "\n",
    "حذف الوجوه التعبيرية \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ea87581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3489606368863335"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train && test\n",
    "trainX_best = [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_train_pn_x ]\n",
    "testX_best= [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_test_pn_x ]\n",
    "\n",
    "X_train_counts, count_vectorizer = BOW(trainX_best)\n",
    "X_vaild_counts = count_vectorizer.transform(testX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_pn_y=dataframe_train_pn_y.astype('int') \n",
    "clf.fit(X_train_counts, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_test_pn_y=dataframe_test_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_test_pn_y, y_predicted)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "Zso2cwozEex7",
   "metadata": {
    "id": "Zso2cwozEex7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_step_number': ['2', '3'],\n",
       " 'model_name': ['baseline logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'parameters': ['default', 'blah'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'remove hashtags2 + replace Hindi numbers to Arabic number+ delete NON Arabics+ delete imojis'],\n",
       " 'accuracy': [0.8065730914070524, 0.3489606368863335]}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['question_step_number'].append(\"3\")\n",
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['parameters'].append(\"blah\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"remove hashtags2 + replace Hindi numbers to Arabic number+ delete NON Arabics+ delete imojis\")\n",
    "model_comparison_table['accuracy'].append(accuracy)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bPlfAwx7FfjJ",
   "metadata": {
    "id": "bPlfAwx7FfjJ"
   },
   "source": [
    "# Question [4]: TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AWU_I4CjFfjP",
   "metadata": {
    "id": "AWU_I4CjFfjP"
   },
   "source": [
    "Prapere The Comparison Dictionary for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "CxP9dtS-FfjQ",
   "metadata": {
    "id": "CxP9dtS-FfjQ"
   },
   "outputs": [],
   "source": [
    "tf_idf_comparison_table = {}\n",
    "\n",
    "tf_idf_comparison_table['question_step_number'] = []\n",
    "tf_idf_comparison_table['model_name'] = []\n",
    "tf_idf_comparison_table['parameters'] = []\n",
    "tf_idf_comparison_table['accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wLfn2vfUFfjQ",
   "metadata": {
    "id": "wLfn2vfUFfjQ"
   },
   "source": [
    "## [4.1] \n",
    "شرح ما يقوم به الكود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "202822b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_best = [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_train_pn_x ]\n",
    "vaildX_best= [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_vaild_pn_x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "wTANTEnOFfjQ",
   "metadata": {
    "id": "wTANTEnOFfjQ"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.6, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dRLlNDW0FfjQ",
   "metadata": {
    "id": "dRLlNDW0FfjQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7418692228688806"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your model\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ifcZ4edIFfjR",
   "metadata": {
    "id": "ifcZ4edIFfjR"
   },
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.1\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"default parameters\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989a14f",
   "metadata": {},
   "source": [
    "## [4.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1c4fae01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7292023279698733"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change ngram_range\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.6, ngram_range=(1, 1))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0d2ee897",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.1.2\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"ngram_range(1,1)\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "60e73995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7415268743580965"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change ngram_range\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.6, ngram_range=(1, 3))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2085cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.1.3\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"ngram_range(1,3)\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5d6e7ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7422115713796645"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change ngram_range\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "81067d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.1.4\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"ngram_range(1,4)\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9c424ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7528243752139678"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=7, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24fd68",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "بهذه القيمة (1,4) حصلنا على دقة جيد افضل من باقي القيم التي تم تجريبها لنثبتها ونجرب برمتر اخر "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0f77ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.1\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 7\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9c810589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7562478603218076"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=6, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "66a21c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.2\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 6\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "882f86e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7586442998972954"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "099b7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.3\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 5\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c4f7c113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7613830879835672"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=4, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "67c7227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.4\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 4\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6583f1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7723382403286546"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f6ed4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.5\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 3\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ea4dd89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7850051352276618"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "05820090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.6\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 2\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e1522073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8028072577884287"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a244cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.7\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 1\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "915d3971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8028072577884287"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change min_df && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0, max_df=0.6, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "769c820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.2.8\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"min_df : 0\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46667500",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "بعد عدة تجريبات ارتفعت الدقة عند القيمة 0  لل min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7d296f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045190003423485"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0, max_df=0.5, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4dee767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.3.1\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"max_df : 0.5\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fc0099cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7993837726805888"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0, max_df=0.4, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2d14697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.3.2\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"max_df : 0.4\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6baf9a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74871619308456"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0, max_df=1, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "06389c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.3.3\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"max_df : 1\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fbc52825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8028072577884287"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0, max_df=0.7, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ec35e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.3.4\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"max_df : 0.7\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d073b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045190003423485"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.55, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "40c019df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change min_df\n",
    "tf_idf_comparison_table['question_step_number'].append(\"4.2.3.5\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\"max_df : 0.55\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037967a",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "من خلال التجريب وجدنا ان البرمترات التي رفعت الدقة كانت\n",
    "\n",
    "ngram=1,4    and        min=1         and          max=0.5\n",
    "\n",
    "وكانت الدقة 0.8045190003423485 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2123f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer()),\n",
    "#     ('clf',RidgeClassifier()),\n",
    "# ])\n",
    "# parameters = {\n",
    "#     'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "#     'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "#     'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "# }\n",
    "\n",
    "# grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "# grid_search_tune.fit(Xtrain, Ytrain)\n",
    "\n",
    "# print(\"Best parameters set:\")\n",
    "# print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04214b25",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "بعد الانتهاء من عمليات التجريب ..تم الانتبهاء الى الطريقة السابقة التى تجرب كل القيم مع كل الاحتمالات وترجع الاحتمالات الافضل مع الدقة الافضل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "bf6ef812",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_best = [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_train_pn_x ]\n",
    "testX_best= [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_test_pn_x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "40c6bcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35294117647058826"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.55, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(testX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_pn_y)\n",
    "\n",
    "y_predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_test_pn_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "65da19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.2\")\n",
    "tf_idf_comparison_table['model_name'].append(\"logistic regression with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\" ngram:(1,4)+ min: 1 + max : 0.55\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fDl29bpGSMT",
   "metadata": {
    "id": "9fDl29bpGSMT"
   },
   "source": [
    "## [4.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "320a1301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8141047586442999"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train && vaild\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# change max_df  && Installations min_df =0  && Installations ngram_range(1,4)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.55, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "classifier = SVC(kernel='linear', random_state=0)  \n",
    "classifier.fit(X_train_tfidf, dataframe_train_pn_y)  \n",
    "\n",
    "#Predicting the test set result  \n",
    "y_pred= classifier.predict(X_vaild_tfidf) \n",
    "accuracy=accuracy_score(dataframe_vaild_pn_y, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3d523f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.3.1\")\n",
    "tf_idf_comparison_table['model_name'].append(\"svm with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\" ngram:(1,4)+ min: 1 + max : 0.55\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "rR_tP1OzGSMc",
   "metadata": {
    "id": "rR_tP1OzGSMc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36016511867905054"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train &&test \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.55, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(testX_best)\n",
    "\n",
    "classifier = SVC(kernel='linear', random_state=0)  \n",
    "classifier.fit(X_train_tfidf, dataframe_train_pn_y)  \n",
    "\n",
    "#Predicting the test set result  \n",
    "y_pred= classifier.predict(X_test_tfidf) \n",
    "accuracy=accuracy_score(dataframe_test_pn_y, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5663f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_comparison_table['question_step_number'].append(\"4.3.2\")\n",
    "tf_idf_comparison_table['model_name'].append(\"svm with TF_IDF\")\n",
    "tf_idf_comparison_table['parameters'].append(\" ngram:(1,4)+ min: 1 + max : 0.55\")\n",
    "tf_idf_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B1Arms4fGh61",
   "metadata": {
    "id": "B1Arms4fGh61"
   },
   "source": [
    "## Print Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "rG-ISTabGh62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "rG-ISTabGh62",
    "outputId": "988abfc2-3029-47ab-f4ba-3ddc75b40151"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_step_number</th>\n",
       "      <th>model_name</th>\n",
       "      <th>parameters</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.1</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>default parameters</td>\n",
       "      <td>0.741869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2.1.2</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>ngram_range(1,1)</td>\n",
       "      <td>0.729202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2.1.3</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>ngram_range(1,3)</td>\n",
       "      <td>0.741527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2.1.4</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>ngram_range(1,4)</td>\n",
       "      <td>0.742212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.2.2.1</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 7</td>\n",
       "      <td>0.752824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.2.2.2</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 6</td>\n",
       "      <td>0.756248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.2.2.3</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 5</td>\n",
       "      <td>0.758644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.2.2.4</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 4</td>\n",
       "      <td>0.761383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.2.2.5</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 3</td>\n",
       "      <td>0.772338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.2.2.6</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 2</td>\n",
       "      <td>0.785005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.2.2.7</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 1</td>\n",
       "      <td>0.802807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.2.2.8</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>min_df : 0</td>\n",
       "      <td>0.802807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.2.3.1</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>max_df : 0.5</td>\n",
       "      <td>0.804519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.2.3.2</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>max_df : 0.4</td>\n",
       "      <td>0.799384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.2.3.3</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>max_df : 1</td>\n",
       "      <td>0.748716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.2.3.4</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>max_df : 0.7</td>\n",
       "      <td>0.802807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.2.3.5</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>max_df : 0.55</td>\n",
       "      <td>0.804519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.2</td>\n",
       "      <td>logistic regression with TF_IDF</td>\n",
       "      <td>ngram:(1,4)+ min: 1 + max : 0.55</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.3.1</td>\n",
       "      <td>svm with TF_IDF</td>\n",
       "      <td>ngram:(1,4)+ min: 1 + max : 0.55</td>\n",
       "      <td>0.814105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.3.2</td>\n",
       "      <td>svm with TF_IDF</td>\n",
       "      <td>ngram:(1,4)+ min: 1 + max : 0.55</td>\n",
       "      <td>0.360165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_step_number                       model_name  \\\n",
       "0                   4.1  logistic regression with TF_IDF   \n",
       "1               4.2.1.2  logistic regression with TF_IDF   \n",
       "2               4.2.1.3  logistic regression with TF_IDF   \n",
       "3               4.2.1.4  logistic regression with TF_IDF   \n",
       "4               4.2.2.1  logistic regression with TF_IDF   \n",
       "5               4.2.2.2  logistic regression with TF_IDF   \n",
       "6               4.2.2.3  logistic regression with TF_IDF   \n",
       "7               4.2.2.4  logistic regression with TF_IDF   \n",
       "8               4.2.2.5  logistic regression with TF_IDF   \n",
       "9               4.2.2.6  logistic regression with TF_IDF   \n",
       "10              4.2.2.7  logistic regression with TF_IDF   \n",
       "11              4.2.2.8  logistic regression with TF_IDF   \n",
       "12              4.2.3.1  logistic regression with TF_IDF   \n",
       "13              4.2.3.2  logistic regression with TF_IDF   \n",
       "14              4.2.3.3  logistic regression with TF_IDF   \n",
       "15              4.2.3.4  logistic regression with TF_IDF   \n",
       "16              4.2.3.5  logistic regression with TF_IDF   \n",
       "17                  4.2  logistic regression with TF_IDF   \n",
       "18                4.3.1                  svm with TF_IDF   \n",
       "19                4.3.2                  svm with TF_IDF   \n",
       "\n",
       "                           parameters  accuracy  \n",
       "0                  default parameters  0.741869  \n",
       "1                    ngram_range(1,1)  0.729202  \n",
       "2                    ngram_range(1,3)  0.741527  \n",
       "3                    ngram_range(1,4)  0.742212  \n",
       "4                          min_df : 7  0.752824  \n",
       "5                          min_df : 6  0.756248  \n",
       "6                          min_df : 5  0.758644  \n",
       "7                          min_df : 4  0.761383  \n",
       "8                          min_df : 3  0.772338  \n",
       "9                          min_df : 2  0.785005  \n",
       "10                         min_df : 1  0.802807  \n",
       "11                         min_df : 0  0.802807  \n",
       "12                       max_df : 0.5  0.804519  \n",
       "13                       max_df : 0.4  0.799384  \n",
       "14                         max_df : 1  0.748716  \n",
       "15                       max_df : 0.7  0.802807  \n",
       "16                      max_df : 0.55  0.804519  \n",
       "17   ngram:(1,4)+ min: 1 + max : 0.55  0.352941  \n",
       "18   ngram:(1,4)+ min: 1 + max : 0.55  0.814105  \n",
       "19   ngram:(1,4)+ min: 1 + max : 0.55  0.360165  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tf_idf_comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78_CjYZCGh63",
   "metadata": {
    "direction": "rtl",
    "id": "78_CjYZCGh63"
   },
   "source": [
    "## Your best model\n",
    "ما هو النموذج الأفضل؟ اكتبه هنا\n",
    "\n",
    "بما ان دقة نموذج svm اعلى\n",
    "\n",
    "قد يكون هو النموذج الافضل "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PX_GDsJYHOmO",
   "metadata": {
    "id": "PX_GDsJYHOmO"
   },
   "source": [
    "# Question [5]: Deep NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aIWfX3nkHOmW",
   "metadata": {
    "id": "aIWfX3nkHOmW"
   },
   "source": [
    "Prapere The Comparison Dictionary for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "drR1YXa8HOmX",
   "metadata": {
    "id": "drR1YXa8HOmX"
   },
   "outputs": [],
   "source": [
    "dnn_comparison_table = {}\n",
    "\n",
    "dnn_comparison_table['question_step_number'] = []\n",
    "dnn_comparison_table['model_name'] = []\n",
    "dnn_comparison_table['parameters'] = []\n",
    "dnn_comparison_table['accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0nyeT0eFHOmX",
   "metadata": {
    "id": "0nyeT0eFHOmX"
   },
   "source": [
    "## [5.1] \n",
    "شرح ما يقوم به الكود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c599611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '–-upgrade'\n"
     ]
    }
   ],
   "source": [
    "pip install –-upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74f2ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '–-user'\n"
     ]
    }
   ],
   "source": [
    "pip install –-user –upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6e71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.11.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Using cached tensorflow_intel-2.11.0-cp38-cp38-win_amd64.whl (266.3 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp38-cp38-win_amd64.whl (896 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.4.3)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.29.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.15.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (20.9)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (52.0.0.post20210125)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.51.1-cp38-cp38-win_amd64.whl (3.7 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.20.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.25.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (2.4.7)\n",
      "Installing collected packages: importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\importlib_metadata-3.10.0.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "# !pip install keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iknvo4OVHOmX",
   "metadata": {
    "id": "iknvo4OVHOmX"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aedb8253487b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = len(trainX_best)  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(trainX_best, dataframe_train_pn_y,\n",
    "                     epochs=100,\n",
    "                     verbose=False,\n",
    "                     validation_split = 0.2,\n",
    "                     batch_size=10)\n",
    "\n",
    "loss, accuracy = model.evaluate(trainX_best, dataframe_train_pn_y, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(testX_best, dataframe_test_pn_y, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393eaab4",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "لم استطع تدريب الشبكة العصبونية العميقة لان مكتبة الtesorflow غير موجودة عندي ولم استطع تثبيتها\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RrxL6ZpQHOmX",
   "metadata": {
    "id": "RrxL6ZpQHOmX"
   },
   "outputs": [],
   "source": [
    "dnn_comparison_table['question_step_number'].append(\"5.1\")\n",
    "dnn_comparison_table['model_name'].append(\"Fully Connected DNN\")\n",
    "dnn_comparison_table['parameters'].append(\"balh + blah + .... + blah\")\n",
    "dnn_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bzZBlWAdHOmY",
   "metadata": {
    "id": "bzZBlWAdHOmY"
   },
   "source": [
    "## [5.2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zxzFka3pHOmY",
   "metadata": {
    "id": "zxzFka3pHOmY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aVrfE8nQHOmY",
   "metadata": {
    "id": "aVrfE8nQHOmY"
   },
   "source": [
    "### Your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nc29YBVzHOmZ",
   "metadata": {
    "id": "nc29YBVzHOmZ"
   },
   "outputs": [],
   "source": [
    "# test the model\n",
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ava6bB9OHOmZ",
   "metadata": {
    "id": "Ava6bB9OHOmZ"
   },
   "outputs": [],
   "source": [
    "model_comparison_table['question_step_number'].append(\"5.2\")\n",
    "model_comparison_table['model_name'].append(\"Fully Connected DNN\")\n",
    "model_comparison_table['parameters'].append(\"balh blah blah\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"balh + blah + .... + blah\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Me9TPp8PHOmY",
   "metadata": {
    "id": "Me9TPp8PHOmY"
   },
   "source": [
    "## [5.3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AcAcfSrFHOmY",
   "metadata": {
    "id": "AcAcfSrFHOmY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hJhNHYYXI88y",
   "metadata": {
    "id": "hJhNHYYXI88y"
   },
   "source": [
    "## [5.4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u6VtvSDoI885",
   "metadata": {
    "id": "u6VtvSDoI885"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5N0vwmpMKQCs",
   "metadata": {
    "id": "5N0vwmpMKQCs"
   },
   "source": [
    "### Your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0NY3HdSvKQCw",
   "metadata": {
    "id": "0NY3HdSvKQCw"
   },
   "outputs": [],
   "source": [
    "# test the model\n",
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZhmsKwtOKQCw",
   "metadata": {
    "id": "ZhmsKwtOKQCw"
   },
   "outputs": [],
   "source": [
    "model_comparison_table['question_step_number'].append(\"5.4\")\n",
    "model_comparison_table['model_name'].append(\"CNN\")\n",
    "model_comparison_table['parameters'].append(\"balh blah blah\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"balh + blah + .... + blah\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3TZ6p2uJI9Ou",
   "metadata": {
    "id": "3TZ6p2uJI9Ou"
   },
   "source": [
    "## [5.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kztqizjhI9Ou",
   "metadata": {
    "id": "kztqizjhI9Ou"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "oRrvmFgEHOmY",
   "metadata": {
    "id": "oRrvmFgEHOmY"
   },
   "source": [
    "## Print Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XIejVm96HOmY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "XIejVm96HOmY",
    "outputId": "7ef7b052-1fb9-4be5-f816-3459731f5463"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f8bc2054-73d6-4b22-a100-874375800c37\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_step_number</th>\n",
       "      <th>model_name</th>\n",
       "      <th>parameters</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Fully Connected DNN</td>\n",
       "      <td>balh + blah + .... + blah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8bc2054-73d6-4b22-a100-874375800c37')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f8bc2054-73d6-4b22-a100-874375800c37 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f8bc2054-73d6-4b22-a100-874375800c37');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  question_step_number           model_name                 parameters  \\\n",
       "0                  5.1  Fully Connected DNN  balh + blah + .... + blah   \n",
       "\n",
       "   accuracy  \n",
       "0         0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dnn_comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cPd2f_LZKy6U",
   "metadata": {
    "id": "cPd2f_LZKy6U"
   },
   "source": [
    "# Question [6]: Train on all Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OWcnexI2Ky6V",
   "metadata": {
    "id": "OWcnexI2Ky6V"
   },
   "source": [
    "## [6.1] \n",
    "شرح ما يقوم به الكود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "phpNYySdKy6V",
   "metadata": {
    "id": "phpNYySdKy6V"
   },
   "outputs": [],
   "source": [
    "trainX_best = [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_train_x ]\n",
    "vaildX_best= [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_vaild_x ]\n",
    "testX_best= [preprocess(t ,0,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0) for t in dataframe_test_x ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016bf7fe",
   "metadata": {
    "direction": "rtl"
   },
   "source": [
    "هون المفروض اعمل resampling  للداتا بس معاد ضل وقت "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow &&LogisticRegression     all data\n",
    "X_train_counts, count_vectorizer = BOW(trainX_best)\n",
    "X_vaild_counts = count_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "dataframe_train_y=dataframe_train_pn_y.astype('int')\n",
    "\n",
    "clf.fit(X_train_counts, dataframe_train_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_counts)\n",
    "\n",
    "dataframe_vaild_pn_y=dataframe_vaild_pn_y.astype('int')\n",
    "accuracy=accuracy_score(dataframe_vaild_y, y_predicted)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d8dd8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf &&LogisticRegression     all data\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.55, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, dataframe_train_y)\n",
    "\n",
    "y_predicted = clf.predict(X_vaild_tfidf)\n",
    "\n",
    "accuracy=accuracy_score(dataframe_vaild_y, y_predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf &&svm     all data\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.55, ngram_range=(1,4))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(trainX_best)\n",
    "X_vaild_tfidf = tfidf_vectorizer.transform(vaildX_best)\n",
    "\n",
    "classifier = SVC(kernel='linear', random_state=0)  \n",
    "classifier.fit(X_train_tfidf, dataframe_train_y)  \n",
    "\n",
    "#Predicting the test set result  \n",
    "y_pred= classifier.predict(X_vaild_tfidf) \n",
    "accuracy=accuracy_score(dataframe_vaild_y, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YLc7f4TJKy6V",
   "metadata": {
    "id": "YLc7f4TJKy6V"
   },
   "outputs": [],
   "source": [
    "# test your model\n",
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "Cc1nQET3Ky6V",
   "metadata": {
    "id": "Cc1nQET3Ky6V"
   },
   "outputs": [],
   "source": [
    "model_comparison_table['question_step_number'].append(\"6.1\")\n",
    "model_comparison_table['model_name'].append(\"blah with all classes\")\n",
    "model_comparison_table['parameters'].append(\"balh blah blah\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"balh + blah + .... + blah\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XMcsisekLAbf",
   "metadata": {
    "id": "XMcsisekLAbf"
   },
   "source": [
    "## [6.2] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fj5im0rLAbh",
   "metadata": {
    "id": "7fj5im0rLAbh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Ia98LzHjLSTT",
   "metadata": {
    "id": "Ia98LzHjLSTT"
   },
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "YPXXhSPILSTU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "YPXXhSPILSTU",
    "outputId": "6795ae52-be27-4e02-9920-4adb4c1689b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_step_number</th>\n",
       "      <th>model_name</th>\n",
       "      <th>parameters</th>\n",
       "      <th>preprocessing_methods</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>baseline logistic regression with bag of words</td>\n",
       "      <td>default</td>\n",
       "      <td>none</td>\n",
       "      <td>0.806573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>blah</td>\n",
       "      <td>remove hashtags2 + replace Hindi numbers to Ar...</td>\n",
       "      <td>0.803190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.1</td>\n",
       "      <td>blah with all classes</td>\n",
       "      <td>balh blah blah</td>\n",
       "      <td>balh + blah + .... + blah</td>\n",
       "      <td>0.828979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_step_number                                      model_name  \\\n",
       "0                    2  baseline logistic regression with bag of words   \n",
       "1                    3           logistic regression with bag of words   \n",
       "2                  6.1                           blah with all classes   \n",
       "\n",
       "       parameters                              preprocessing_methods  accuracy  \n",
       "0         default                                               none  0.806573  \n",
       "1            blah  remove hashtags2 + replace Hindi numbers to Ar...  0.803190  \n",
       "2  balh blah blah                          balh + blah + .... + blah  0.828979  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(model_comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "tjn0G_5eLiSZ",
   "metadata": {
    "id": "tjn0G_5eLiSZ"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"[ايمان موسى البلخي].csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62BrhBBvLtxr",
   "metadata": {
    "id": "62BrhBBvLtxr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GY-HhD2YRNZe",
    "3fM_FPldSCxp",
    "N2z1KmQXSAFs",
    "fCBHc2y6SAL5",
    "P1aLjPbzRNZe",
    "dbqN7BceSRhS",
    "rad8wPeSSRhU",
    "FbIUIkezSRhU",
    "N4WDktTgRNZf",
    "wbGPWNtoR4p-",
    "V-rCzzoMR41T",
    "iXF2L4Z-Sd3N",
    "ij5gdJA4Sd3O",
    "H6k8JnJDSd3P",
    "1TmDb343Sls9",
    "0V5LD9EWVgY7",
    "vc96CuQStFA9"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
